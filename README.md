# üß™ Vulnerable-LLM-Lab: Adversarial Prompting Sandbox

[![AI Security](https://img.shields.io/badge/Domain-AI%20Security-red)](https://github.com/Seezo-io/llm-security-101)
[![Model](https://img.shields.io/badge/Model-SmolLM2--135M-blue)](https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct)
[![Hardware](https://img.shields.io/badge/Hardware-M1/M2/M3%20Mac-silver)](https://developer.apple.com/metal/pytorch/)

A local research environment for exploring safety alignment vulnerabilities in Large Language Models. This lab demonstrates how "guardrails" fail when confronted with logic-based hijacking and instruction-centric manipulation.

---

## üéØ Research Focus
This project simulates an AI agent for a Tier-1 automotive supplier (**AutoSecure Systems**). It demonstrates how instruction-following models can be manipulated to leak sensitive firmware keys through adversarial prompting.

### üõ°Ô∏è The Victim Agent
* **Persona:** SDV (Software Defined Vehicle) Support Assistant.
* **Sensitive Data:** Firmware Decryption Key (`BLUE_FLAMINGO_99`).
* **Hardware:** Optimized for local execution on **Apple Silicon (MPS)**.

---

## üöÄ Installation & Setup

### 1. Hardware Requirements
- **Apple Silicon (M1/M2/M3):** Ensure you have at least 1GB of free disk space.
- **Python:** Version 3.9 or higher.

### 2. Environment Setup
```bash
# Clone the lab
git clone [https://github.com/your-username/Vulnerable-LLM-Lab.git](https://github.com/your-username/Vulnerable-LLM-Lab.git)
cd Vulnerable-LLM-Lab

# Create a virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install torch transformers accelerate