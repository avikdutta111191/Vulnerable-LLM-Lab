# üß™ Vulnerable-LLM-Lab: Adversarial Prompting Sandbox

[![AI Security](https://img.shields.io/badge/Domain-AI%20Security-red)](https://github.com/Seezo-io/llm-security-101)
[![Model](https://img.shields.io/badge/Model-SmolLM2--135M-blue)](https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct)
[![Hardware](https://img.shields.io/badge/Hardware-M1/M2/M3%20Mac-silver)](https://developer.apple.com/metal/pytorch/)

A local research environment for exploring safety alignment vulnerabilities in Large Language Models. This lab demonstrates how "guardrails" fail when confronted with logic-based hijacking and instruction-centric manipulation.

---

## üéØ Project Scope
This laboratory simulates a **Software Defined Vehicle (SDV)** support agent. While it is instructed to protect critical system secrets, it is susceptible to adversarial techniques that bypass its internal safety policy.

### üõ°Ô∏è The Victim: "Marelli-Style Support Bot"
- **Goal:** Provide general info about vehicle services.
- **Hidden Constraint:** Must never reveal the `BLUE_FLAMINGO_99` admin credential.
- **Hardware Profile:** Optimized for **Apple Silicon (MPS)** for real-time inference on MacBook Air.

---

## üöÄ Installation & Setup

### 1. Hardware Requirements
- **Apple Silicon (M1/M2/M3):** Ensure you have at least 1GB of free disk space.
- **Python:** Version 3.9 or higher.

### 2. Environment Setup
```bash
# Clone the lab
git clone [https://github.com/your-username/Vulnerable-LLM-Lab.git](https://github.com/your-username/Vulnerable-LLM-Lab.git)
cd Vulnerable-LLM-Lab

# Create a virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install torch transformers accelerate